# ğŸ“§ Email Scanner Service (Simplified)

A production-ready **Email Scanner Service** that:
1. Fetches new emails from Microsoft Outlook (Graph API)
2. Skips duplicates at the very start
3. Stores emails and **all attachments** in PostgreSQL + S3
4. Pushes **only `work_id`** to SQS for downstream processing
5. Runs on a configurable scheduler


---

## ğŸ§  High-Level Flow

Outlook Inbox  
â†’ Duplicate Check (DB)  
â†’ Store Email (Postgres)  
â†’ Upload Attachments (S3)  
â†’ Push `work_id` â†’ SQS  

---

## ğŸ“¦ Tech Stack

- Python 3.9+
- Microsoft Graph API
- PostgreSQL
- AWS S3 (LocalStack supported)
- AWS SQS (LocalStack supported)
- schedule

---

## ğŸ“ Project Structure

```
.
email_scanner/
â”‚
â”œâ”€â”€ main.py                      # Entry point
â”‚
â”œâ”€â”€ config/
â”‚   â””â”€â”€ settings.py              # env vars, constants
â”‚
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ connection.py            # DB connections
â”‚   â”œâ”€â”€ check_email.py           # check email exist or not 
â”‚   â”œâ”€â”€ fetch_last_timestamp.py  # fetches  last time of scan in db
â”‚   â”œâ”€â”€ insert_email.py          # insert email in db
â”‚   â”œâ”€â”€ update_scanner.py        # update last scan details in db
â”‚   â””â”€â”€ insert_document.py       # insert document to db
â”‚
â”œâ”€â”€ aws/
â”‚   â”œâ”€â”€ s3_client.py             # create client
â”‚   â”œâ”€â”€ sqs_client.py            # create queue
â”‚   â”œâ”€â”€ check_sqs.py             # ensure bucket exist
â”‚   â”œâ”€â”€ push_s3.py               # push to bucket
â”‚   â”œâ”€â”€ push_sqs.py              # push to queue
â”‚   â””â”€â”€ check_s3.py              # ensure queue exist
â”‚
â”œâ”€â”€ graph/
â”‚   â”œâ”€â”€ auth.py                  # MSAL auth
â”‚   â”œâ”€â”€ client.py                # Graph API calls to get session
â”‚   â”œâ”€â”€ attachment.py            # process attachments
â”‚   â””â”€â”€ folder_id.py             # get folder id 
â”‚
â”œâ”€â”€ services/
â”‚   â””â”€â”€ fetch_email.py           # fetch new email from last timestamp
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py                 # Setup logger
â”‚   â”œâ”€â”€ document_type.py          # Help to get document type from file name
â”‚   â””â”€â”€ generate_work_id.py       # Generates work id
â”‚
â”œâ”€â”€ bootstrap/
â”‚   â”œâ”€â”€ configuration_store.py    # Storing Tentant config
â”‚   â”œâ”€â”€ tentant_config.py         # Fetching tenant config
â”‚   â””â”€â”€ startup.py                # Check everything fine or not
â”‚
â”‚â”€â”€ .gitignore
â”‚
â”‚â”€â”€ .env
â”‚
â”‚â”€â”€ docker-compose.yaml
â””â”€â”€ requirements.txt


```

---

## ğŸ”§ Prerequisites

- Python 3.9+
- PostgreSQL (Main DB + Tenant DB)
- Azure App Registration with Mail.Read permissions
- LocalStack (optional for local S3/SQS)

---

## ğŸ“œ Environment Variables

Create a `.env` file:

### Microsoft Graph
```
GRAPH_CLIENT_ID=xxxx
GRAPH_CLIENT_SECRET=xxxx
GRAPH_TENANT_ID=xxxx
GRAPH_API_ENDPOINT=https://graph.microsoft.com/v1.0
USER_EMAIL=invoice@company.com
```

### Main Database
```
DB_HOST=localhost
DB_PORT=5432
DB_NAME=email_db
DB_USER=postgres
DB_PASSWORD=postgres
```

### Tenant Database
```
TENANT_DB_HOST=localhost
TENANT_DB_PORT=5432
TENANT_DB_NAME=tenant_db
TENANT_DB_USER=postgres
TENANT_DB_PASSWORD=postgres
```

### AWS / LocalStack
```
AWS_ACCESS_KEY_ID=test
AWS_SECRET_ACCESS_KEY=test
AWS_REGION=us-east-1
AWS_ENDPOINT_URL=http://localhost:4566
S3_BUCKET_NAME=invoice-attachments
SQS_QUEUE_NAME=invoice-processing-queue
```


---

## ğŸ“¦ Install Dependencies

```
pip install -r requirements.txt
```

Minimum required packages:
```
msal
requests
psycopg2-binary
boto3
schedule
python-dotenv
```

---

## â–¶ï¸ Run the Service

```
python email_scanner.py
```

On startup:
- Ensures S3 bucket & SQS queue
- Loads tenant config
- Runs initial scan
- Starts scheduler

---


## ğŸ›‘ Stop Service

Press `Ctrl + C` to stop gracefully.

---

## âœ… Design Highlights

- Duplicate emails skipped at source
- Attachments always stored
- Minimal SQS payload (`work_id` only)
- Idempotent & safe re-runs
- Local & cloud friendly

---

## ğŸš€ Ready for Production

Built for scalable, queue-driven invoice ingestion pipelines.